{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will go trough an PyTorch implementation of transformer based on [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper.\n",
    "\n",
    "First, I will implement just the Encoder part and train it on language modeling task which is predicting the next word in the sentence given the beginning. The model will be able to generate new sentences once it's trained. Then I will add the Decoder and train the whole Transformer on machine translation task. \n",
    "\n",
    "Troughout this notebook I **use some of the illustrations from the exceptional [Illustrated transformer](https://jalammar.github.io/illustrated-transformer) blog post by *Jay Alammar***. I highly recommend you to take a look at it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For language modeling task I will use english sentences from a small [Eng-Fra](https://raw.githubusercontent.com/mf1024/transformers/master/fra-eng/fra.txt) dataset. The dataset consists of 170k sentences.\n",
    "\n",
    "I wrote an pytorch [Dataset](https://github.com/mf1024/transformers/blob/master/fra_eng_dataset.py) to process the sentences. I tokenize the sentences using word_tokenizer from nltk library, nothing too fancy. \n",
    "\n",
    "Here are some sentence exeamples:\n",
    "\n",
    ">**He said nothing, which made her angry.**\n",
    "\n",
    ">**We have six lessons a day.**\n",
    "\n",
    ">**Were you able to solve the problem?**\n",
    "\n",
    ">**How long does it take to get to the stadium?**\n",
    "\n",
    ">**No matter how long it takes, I will finish the work.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Language modeling\n",
    "\n",
    "Language modeling is predicting the probabilities of the next word given the previous words in the sequence. \n",
    "With a slight modification the Transformer Encoder can be trained on the Language modeling task. \n",
    "\n",
    "The input of the Encoder is the whole sentence and the targeted output is the sentence shifted by one so that the corresponding output for each word is the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture:\n",
    "\n",
    "<img src=\"imgs/transformer.png\">\n",
    "\n",
    "The key idea of Transformer is to avoid using [reccurence](https://arxiv.org/abs/1409.3215) at all for encoding and decoding variable-length sequences. That solves issues with long-range dependencies and the amount of computation that can be parallelized.\n",
    "\n",
    "The transformer consists of two parts - the Encoder and the Decoder. They are both very similar in their structure but different in few aspects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"imgs/encoder.png\">\n",
    "\n",
    "The first Encoder layer reads the embedding vectors of the whole sentence, applies self-attention to the sentence, does feed-forward transformation on each of the vectors and outputs vectors of the exactly same dimensions. The second layer does the same to the first layer outputs and so on until the last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Self-Attention\n",
    "\n",
    "The first step in the self-attention is to calculate the **Key**, **Value** and **Query** vectors for each of the embedding vector. It is done by applying linear transformation on the embedding vector. \n",
    "\n",
    "Then for each embedding the **Query** vector is used to find the score with other **Keys** in the sentence or in other words what amount of attention we want to pay to other embeddings int the sentence. Then the similarity of the **Query** and **Keys** of other words in the sentence turns into attention vector of weights which tells how much of the **Value** of the corresponding word of we want to factor in into the new embedding value. The new embedding value is weighted sum of all word **Values** form the sentence.\n",
    "\n",
    "<img src=\"imgs/selfattention.png\">\n",
    "\n",
    "These operations can be vectorized and done in a few matrix operations where **Q,K** and **V** (queries, keys, values in the code) are matrices containing Queries, Keys and Values for each of the word in the sentence and Z contains the resulting embeddings:\n",
    "\n",
    "<img src=\"imgs/vectorized_self_attention.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking for language modeling\n",
    "\n",
    "There is one small modification I've made to the Encoder so that it can be used for the language modelling task - subsequent Value element masking, which allows self-attention to attend only to current and the previous words in the sentence. The input of the Encoder is the whole sentence and the targeted output is the sentence shifted by one so that the corresponding output for each word is the next word. Without masking the subsequent words the model could simply attend to the next word in the input and output it but with the masking it has to figure out the next word from the previous sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "        # src shape: [N, SEQ, D_MODEL]\n",
    "\n",
    "        keys = self.K.forward(src)\n",
    "        values = self.V.forward(src)\n",
    "        queries = self.Q.forward(src)\n",
    "        \n",
    "        sqrt_d = self.d_model ** 0.5\n",
    "\n",
    "        att = torch.matmul(queries, keys.transpose(1,2)) / sqrt_d\n",
    "        # att shape: [N, SEQ, SEQ]\n",
    "        # Broadcast padding mask to word attentions so that word attention does not attend to positions outside the sentence\n",
    "        att = att + src_padding_mask.transpose(1,2)\n",
    "        # Add subsequent mask so that each position can attend only itself and the previous elements\n",
    "        att = att + src_subsq_mask.unsqueeze(0)\n",
    "        att_softmax = torch.softmax(att, dim=2)\n",
    "        # att_softmax shape: [N, SEQ, SEQ]\n",
    "        out = torch.matmul(att_softmax, values)\n",
    "        # out shape: [N, SEQ, D_MODEL]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead attention\n",
    "\n",
    "Multihead attention is concatenation of many self-attention instances. The intuition of multihead attention is that each of the heads can learn to pay attention to different types of things.\n",
    "\n",
    "The outputs of the heads are concatinated and then linearly transformed into regular sized embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(d_model) for i in range(num_heads)])\n",
    "        self.linear = nn.Linear(num_heads * d_model, d_model)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        out_cat = None\n",
    "        for i in range(self.num_heads):\n",
    "            if i == 0:\n",
    "                out_cat = self.heads[i].forward(src, src_padding_mask, src_subsq_mask)\n",
    "            else:\n",
    "                out_cat = torch.cat([out_cat, self.heads[i].forward(src, src_padding_mask, src_subsq_mask)], dim=2)\n",
    "\n",
    "        ret = self.linear.forward(out_cat)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_att_heads, ff_dim = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_att_heads)\n",
    "        self.att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.lin_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        #Self Attention sub-block      \n",
    "        res1 = src #Storing tmp values for residual connection\n",
    "        x = self.multihead_attention.forward(src, src_padding_mask, src_subsq_mask)\n",
    "        x = self.att_sublayer_norm.forward(res1 + self.dropout1(x)) #Dropout, residual connection and normalization\n",
    "\n",
    "        #Feed-Forward sub-block    \n",
    "        res2 = x #Storing tmp values for residual connection\n",
    "        x = self.linear2(self.relu(self.linear1.forward(x)))\n",
    "        x = self.lin_sublayer_norm(res2 + self.dropout2(x)) #Dropout, residual connection and normalization\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_att_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_att_heads) for i in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,src, src_padding_mask, src_subsq_mask):\n",
    "        x = src\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, src_padding_mask, src_subsq_mask)\n",
    "\n",
    "        x = self.norm.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The attention module itself have no information about the position of other word embeddings in the sentence. In NLP and other sequential data tasks the order of the sequence is critical. To solve this issue the authors of the paper introduce the Positional Encoding method. First they generate a vector containing information about the position in the same size as the embedding and then simply add this vector to the word embedding and hope that the model will learn to recognize it.\n",
    "\n",
    "They use the following function to generate the positional information: \n",
    "<img src=\"imgs/pos_enc.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.sin_args = torch.zeros(1, self.d_model).to(device)\n",
    "        self.cos_args = torch.zeros(1, self.d_model).to(device)\n",
    "        for i in range(self.d_model//2):\n",
    "            self.sin_args[0,i * 2] = 10000**(2.*i/self.d_model)\n",
    "            self.cos_args[0,i * 2 + 1] = 10000**(2.*i/self.d_model)\n",
    "\n",
    "        self.sin_args_mask = (self.sin_args > 1e-10).float()\n",
    "        self.sin_args = self.sin_args + (self.sin_args < 1e-10).float()\n",
    "\n",
    "        self.cos_args_mask = (self.cos_args > 1e-10).float()\n",
    "        self.cos_args = self.cos_args + (self.cos_args < 1e-10).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for pos in range(x.size()[-2]):\n",
    "            x[:,pos,:] = x[:,pos,:] + \\\n",
    "                         torch.sin(pos / self.sin_args) * self.sin_args_mask + \\\n",
    "                         torch.cos(pos / self.cos_args) * self.cos_args_mask\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a vizualization of the positional encoding vectors. Each row is an encoding for a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "positional_enc = PositionalEncoding(128).to(device)\n",
    "data = torch.zeros(1, 50, 128).to(device)\n",
    "data_pos_enc = positional_enc.forward(data)\n",
    "\n",
    "enc_np = data_pos_enc.squeeze(dim=0).to('cpu').numpy()\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.imshow(enc_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_att_heads, input_dict_size, output_dict_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_emb = nn.Embedding(input_dict_size, d_model)\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder = Encoder(num_layers, d_model, num_att_heads)\n",
    "        self.decoder = None\n",
    "\n",
    "        self.outp_logits = nn.Linear(d_model, output_dict_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        x = self.input_emb.forward(src.squeeze(dim=2))\n",
    "        x = self.positional_encoder.forward(x)\n",
    "\n",
    "        x = self.encoder.forward(x, src_padding_mask, src_subsq_mask)\n",
    "        x = self.outp_logits.forward(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square_subsequent_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len).to(device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def get_padding_mask(input, val1 = float('-inf'), val2 = float(0.0)):\n",
    "    mask = torch.ones(input.size()).to(device)\n",
    "    mask = mask.float().masked_fill(input == 0, val1).masked_fill(input > 0, val2)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_one_hot(x, out_dim, mask):\n",
    "    tens = x.view(-1)\n",
    "    tens_one_hot = torch.zeros(list(tens.size()) + [out_dim]).to(device)\n",
    "    for i in range(len(tens)):\n",
    "        tens_one_hot[i,tens[i]] = 1\n",
    "\n",
    "    tens_one_hot = tens_one_hot.view(list(x.size()) + [out_dim])\n",
    "    tens_one_hot = tens_one_hot * mask\n",
    "    return tens_one_hot.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition, Dataset, optimizer and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170190\n"
     ]
    }
   ],
   "source": [
    "from fra_eng_dataset import FraEngDataset, fra_eng_dataset_collate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "STORE_MODELS = True\n",
    "models_path = 'models'\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "dataset = FraEngDataset()\n",
    "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=fra_eng_dataset_collate)\n",
    "\n",
    "in_dict_size = dataset.get_eng_dict_size()\n",
    "\n",
    "transformer_model = Transformer(\n",
    "    num_layers=6,\n",
    "    d_model=512,\n",
    "    num_att_heads=8,\n",
    "    input_dict_size=in_dict_size,\n",
    "    output_dict_size=in_dict_size # We do language modeling so we will use in_dict_size for output as well\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate senteces one by one using our trained model\n",
    "Will use it after each epoch to see how good the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_some_sentences(num_sentences = 25):\n",
    "\n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_sentences):\n",
    "            snt = torch.ones((1,1,1)).to(device) * dataset.get_eng_start_code()\n",
    "            snt = snt.long()\n",
    "\n",
    "            sent_idxes = []\n",
    "\n",
    "            for i in range(25):\n",
    "                pred = transformer_model.forward(\n",
    "                    src = snt,\n",
    "                    src_padding_mask = torch.zeros_like(snt).float().to(device),\n",
    "                    src_subsq_mask = get_square_subsequent_mask(snt.size()[1]),\n",
    "                )\n",
    "                next_word_softmax = pred[0,i,:].to('cpu').detach().numpy()\n",
    "                next_word = np.random.choice(len(next_word_softmax), p=next_word_softmax)\n",
    "                snt = torch.cat([snt, torch.ones((1,1,1)).long().to(device) * next_word], dim=1)\n",
    "\n",
    "                sent_idxes.append(next_word)\n",
    "\n",
    "                if next_word == dataset.get_eng_eos_code():\n",
    "                    break\n",
    "\n",
    "            sent = ''\n",
    "            for word_idx in sent_idxes:\n",
    "                sent = f\"{sent} {dataset.eng_token_to_text[word_idx]}\"\n",
    "\n",
    "            print(sent)\n",
    "\n",
    "    transformer_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ============================================================\n",
      "Total loss per word: 3.8989408016204834\n",
      "Some generated sentences:\n",
      " Had ever cut Super off when we unwrapped the rain . <EOS>\n",
      " I overstepped the interesting to anabolic two there . <EOS>\n",
      " He walked to eat a friend . <EOS>\n",
      " Grab ? <EOS>\n",
      " What do we have ? <EOS>\n",
      " I 'd cried if this is expert . <EOS>\n",
      " He asked up when he managed lunch for a accompanied . <EOS>\n",
      " Where do you need my proposal ? <EOS>\n",
      " You 're books . <EOS>\n",
      " I needed to visit an Here . <EOS>\n",
      " Do n't give me a lot . <EOS>\n",
      " That 's not not more hungry than we can do . <EOS>\n",
      " Well , have we ? <EOS>\n",
      " I 've never wonder what was doing . <EOS>\n",
      " Since , I 'd have been alone . <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Transformer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MultiHeadAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/martin/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SelfAttentionHead. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ============================================================\n",
      "Total loss per word: 3.268401861190796\n",
      "Some generated sentences:\n",
      " He nodded all morning . <EOS>\n",
      " How much do you mind to hear that for ? <EOS>\n",
      " She 's not as nice as I really could . <EOS>\n",
      " The prime put changed with birthdays . <EOS>\n",
      " I 'm hungry . <EOS>\n",
      " Ask me the last way he does . <EOS>\n",
      " The rebels handed cheated a lot like it rains . <EOS>\n",
      " I 've been to foot for dinner . <EOS>\n",
      " Tom meant something that somebody will be connected to me . <EOS>\n",
      " He told me what else to do . <EOS>\n",
      " I can hear this game . <EOS>\n",
      " I was grammar old impression of a chance . <EOS>\n",
      " I do n't really care . <EOS>\n",
      " Part goes out ! <EOS>\n",
      " I fell for the bicycle . <EOS>\n",
      "Epoch 2 ============================================================\n",
      "Total loss per word: 3.05794620513916\n",
      "Some generated sentences:\n",
      " This letter is received startling . <EOS>\n",
      " Do you have something to eat to eat ? <EOS>\n",
      " Do you want friends ? <EOS>\n",
      " You support a brother . <EOS>\n",
      " I joystick I 'm entirely not a doctor 's license . <EOS>\n",
      " You 've got such a little shame . <EOS>\n",
      " The girl gave him another word a pep attack . <EOS>\n",
      " Tom 's cat are familiar with a fool . <EOS>\n",
      " He 's a n't poor . <EOS>\n",
      " We always have to make help you . <EOS>\n",
      " Just put him back in boiling cookies . <EOS>\n",
      " These are the peanuts to pick up up up . <EOS>\n",
      " I am getting proud of your punishing away . <EOS>\n",
      " I remember writing her at the concert . <EOS>\n",
      " You 're the only one who trained me . <EOS>\n",
      "Epoch 3 ============================================================\n",
      "Total loss per word: 2.908892869949341\n",
      "Some generated sentences:\n",
      " The cops were looking for a few minutes . <EOS>\n",
      " Did they read the book ? <EOS>\n",
      " My eyes are clear . <EOS>\n",
      " He looks really happy . <EOS>\n",
      " Over people say the truth . <EOS>\n",
      " Please say that . <EOS>\n",
      " All Tom broke . <EOS>\n",
      " Thanks to the meeting , I could do more as to make sure how to learn . <EOS>\n",
      " We should 've met a man outside . <EOS>\n",
      " How many times give up to Tom ? <EOS>\n",
      " It 's not necessary to explain this immediately . <EOS>\n",
      " The few days put the square Tower software . <EOS>\n",
      " Let 's cut a tree after miniature . <EOS>\n",
      " My mom wo n't kill you . <EOS>\n",
      " Is this your sister ? <EOS>\n",
      "Epoch 4 ============================================================\n",
      "Total loss per word: 2.7897863388061523\n",
      "Some generated sentences:\n",
      " I 'm glad you asked that how carefully . <EOS>\n",
      " After we had over the plan , we had been advised to go a house . <EOS>\n",
      " I ca n't be used to this dog . <EOS>\n",
      " It was the best of his company . <EOS>\n",
      " How could you be so cruel ? <EOS>\n",
      " I 've protected a tough dictionary . <EOS>\n",
      " What kind of medicine does you like ? <EOS>\n",
      " I caught something where we 'd be today . <EOS>\n",
      " There are many lined that can take out through chill . <EOS>\n",
      " I 'd like my book . <EOS>\n",
      " You may choose one of them . <EOS>\n",
      " We were late for the first meeting . <EOS>\n",
      " I think it makes a good slight tick use of him . <EOS>\n",
      " I broke the problems . <EOS>\n",
      " I 'm not sure what it would be like . <EOS>\n",
      "Epoch 5 ============================================================\n",
      "Total loss per word: 2.689284324645996\n",
      "Some generated sentences:\n",
      " I 'm not coming far . <EOS>\n",
      " Ask me if you want to see the jello . <EOS>\n",
      " Each child is as beautiful as poets . <EOS>\n",
      " Who would have read that subject ? <EOS>\n",
      " Could you direct me to the supermarket ? <EOS>\n",
      " Tom is extremely busy that he owes me . <EOS>\n",
      " We 're too happy to help . <EOS>\n",
      " I thought there could sleep on the window . <EOS>\n",
      " I just want you to think I 'm fair . <EOS>\n",
      " Tom must 've done that . <EOS>\n",
      " I drove through an appointment here . <EOS>\n",
      " The plane was filled with this church fruits . <EOS>\n",
      " What I met him in the morning ? <EOS>\n",
      " I want to stay around here . <EOS>\n",
      " I ca n't believe I 'm coming this this ? <EOS>\n",
      "Epoch 6 ============================================================\n",
      "Total loss per word: 2.601416826248169\n",
      "Some generated sentences:\n",
      " I picked you up . <EOS>\n",
      " It 's not a crime . <EOS>\n",
      " She should n't have my photograph license . <EOS>\n",
      " It 'll be bad for you , it 's not steals possible . <EOS>\n",
      " Several of these are mine . <EOS>\n",
      " I do n't understand what you 're worrying . <EOS>\n",
      " Tom knew afterwards inside in Arabic . <EOS>\n",
      " Where did the program work ? <EOS>\n",
      " I followed up very fast . <EOS>\n",
      " Tom did n't know Mary were fries at her party . <EOS>\n",
      " That 's clever . <EOS>\n",
      " It was n't a mistake . <EOS>\n",
      " You did n't vote . <EOS>\n",
      " My significant are different of things . <EOS>\n",
      " How ever do you know Tom is a general artist ? <EOS>\n",
      "Epoch 7 ============================================================\n",
      "Total loss per word: 2.522509813308716\n",
      "Some generated sentences:\n",
      " Who 's to tell you what really drinking ? <EOS>\n",
      " Let 's ignore this bracelet up and you wo n't even let them work in French . <EOS>\n",
      " I think you 're just thirty minutes . <EOS>\n",
      " Unfortunately , Tom is n't Mary ? <EOS>\n",
      " I 'll tell you where to look . <EOS>\n",
      " Please stop playing there . <EOS>\n",
      " He will never be able to speak more directly . <EOS>\n",
      " The army was just upstairs . <EOS>\n",
      " Why did you take that good chance ? <EOS>\n",
      " This was anyone here . <EOS>\n",
      " Is that actually simple , makes you happy ? <EOS>\n",
      " I 'm so proud of our team . <EOS>\n",
      " Thank you for coming and see me again . <EOS>\n",
      " She ran away from her house . <EOS>\n",
      " Wipe your room and painting the puzzle . <EOS>\n",
      "Epoch 8 ============================================================\n",
      "Total loss per word: 2.4509177207946777\n",
      "Some generated sentences:\n",
      " This box is growing the ball . <EOS>\n",
      " You are not a doctor , exactly a child . <EOS>\n",
      " You ca n't believe everything you have says . <EOS>\n",
      " I put the box on the desk . <EOS>\n",
      " We need to be bodyguard to do that . <EOS>\n",
      " It never makes a secret . <EOS>\n",
      " I 've been writing a letter . <EOS>\n",
      " General production of plants . <EOS>\n",
      " They must find a place to pull the rules . <EOS>\n",
      " You guessed right . <EOS>\n",
      " Have you ever been back in France ? <EOS>\n",
      " Would you like all these consumption ? <EOS>\n",
      " Is that a picture of a cat or anything ? <EOS>\n",
      " What do you want to be desired ? <EOS>\n",
      " Tom does n't like Mary . <EOS>\n",
      "Epoch 9 ============================================================\n",
      "Total loss per word: 2.3850018978118896\n",
      "Some generated sentences:\n",
      " I was just about to call you before the last time . <EOS>\n",
      " My uncle lives in a cozy town . <EOS>\n",
      " Why do n't you talk to me now ? <EOS>\n",
      " He was Remote raised . <EOS>\n",
      " You 're not wearing a hat . <EOS>\n",
      " She became a dancer . <EOS>\n",
      " Ca n't you do anything ? <EOS>\n",
      " It 's easy to add a password which one by old . <EOS>\n",
      " I 'm not afraid of people except you like that . <EOS>\n",
      " I do n't know why I bother doing that . <EOS>\n",
      " If you could go , , I would accept . <EOS>\n",
      " You 're very wise . <EOS>\n",
      " He was the first to lose . <EOS>\n",
      " She 's the one who planted that tree . <EOS>\n",
      " How much will the Beatles last ? <EOS>\n",
      "Epoch 10 ============================================================\n",
      "Total loss per word: 2.323307752609253\n",
      "Some generated sentences:\n",
      " She has no right to answer . <EOS>\n",
      " You sort of hinted that you want me to be rich . <EOS>\n",
      " Put this seat in his tea . <EOS>\n",
      " Planes are used to salesgirl . <EOS>\n",
      " Thanks for all the letters that he told me to give for such a job . <EOS>\n",
      " She is very tall . <EOS>\n",
      " I 'll try it again forever . <EOS>\n",
      " My cat alarm , Tom and I are in the same bed . <EOS>\n",
      " I did n't realize Tom had told us that it was the truth . <EOS>\n",
      " Tom hates to cry . <EOS>\n",
      " Tom died and Mary checked 30 dollars . <EOS>\n",
      " Someone is watching Tom . <EOS>\n",
      " Please mind what I said . <EOS>\n",
      " I 've got to go fishing with my family . <EOS>\n",
      " Japanese ride is a cat . <EOS>\n",
      "Epoch 11 ============================================================\n",
      "Total loss per word: 2.268059492111206\n",
      "Some generated sentences:\n",
      " Maybe she can sing . <EOS>\n",
      " Tom may give a hook to anyone . <EOS>\n",
      " I remember spectators against her . <EOS>\n",
      " I 've forgotten your name . <EOS>\n",
      " What 's your favorite kind of food ? <EOS>\n",
      " He turned his attention to his father 's eggs . <EOS>\n",
      " I am just a kid . <EOS>\n",
      " Your books will solve the visit tomorrow . <EOS>\n",
      " I had a tough dream . <EOS>\n",
      " Tom forgot some mugs on me . <EOS>\n",
      " Tom is alone in his room . <EOS>\n",
      " I 'm decorating the attic . <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I wish I 'd told you everything should have done more . <EOS>\n",
      " I had to meet him . <EOS>\n",
      " They wo n't tell you the truth . <EOS>\n",
      "Epoch 12 ============================================================\n",
      "Total loss per word: 2.216280937194824\n",
      "Some generated sentences:\n",
      " I 'm ready for Monday . <EOS>\n",
      " Something might be dorm , so tell me no wonder what you 're talking about . <EOS>\n",
      " You told me that you were busy . <EOS>\n",
      " We ca n't fix this anymore . <EOS>\n",
      " Tom is still jealous . <EOS>\n",
      " I 'm not in good health . <EOS>\n",
      " My phone is ruined again . <EOS>\n",
      " My uncle is intelligent . <EOS>\n",
      " I am sorry , I did n't recognize you . <EOS>\n",
      " Are you the one who caused this mess ? <EOS>\n",
      " I need to give you some declined . <EOS>\n",
      " He paid two dollars off his telephone 's cigarette . <EOS>\n",
      " Do you remember when we first met ? <EOS>\n",
      " Do you have a flashlight I can write of sake ? <EOS>\n",
      " The news is spacious on the ground . <EOS>\n",
      "Epoch 13 ============================================================\n",
      "Total loss per word: 2.1666030883789062\n",
      "Some generated sentences:\n",
      " Tom sat on his bench . <EOS>\n",
      " Take away your car and do n't know what I 'm talking about . <EOS>\n",
      " I know you need to speak to Tom . <EOS>\n",
      " Where would it be ? <EOS>\n",
      " This was my first choice . <EOS>\n",
      " I just do n't know what to do . <EOS>\n",
      " Let me help you with your homework . <EOS>\n",
      " He is no saint . <EOS>\n",
      " He told his father that he was staying friendly . <EOS>\n",
      " You 're the only one here for the candidate . <EOS>\n",
      " She kissed her cheek in the bathtub . <EOS>\n",
      " There 's another way to do that . <EOS>\n",
      " I had no intention of smoking happen . <EOS>\n",
      " You got ta get more organized . <EOS>\n",
      " I have no insurance . <EOS>\n",
      "Epoch 14 ============================================================\n",
      "Total loss per word: 2.120757579803467\n",
      "Some generated sentences:\n",
      " I was not very athletic then . <EOS>\n",
      " You are ruthless . <EOS>\n",
      " I 've never seen that guy before . <EOS>\n",
      " She 's wearing no dress , is she ? <EOS>\n",
      " Put the well-trained in the form . <EOS>\n",
      " I 've asked you a stupid question . <EOS>\n",
      " Nothing used to eat a lot . <EOS>\n",
      " I got there in time for the meeting . <EOS>\n",
      " She 's many difficulties . <EOS>\n",
      " Please show me what I want . <EOS>\n",
      " Tom is a difficult person to do that . <EOS>\n",
      " I 'm ashamed that my father will give me a birthday birthday coat . <EOS>\n",
      " I have n't had time to watch advertising and lunch . <EOS>\n",
      " I do n't know about you , but I 'm starved . <EOS>\n",
      " One of my friends laughed . <EOS>\n",
      "Epoch 15 ============================================================\n",
      "Total loss per word: 2.0783469676971436\n",
      "Some generated sentences:\n",
      " Here 's Tom 's address . <EOS>\n",
      " He fixed that we have no choice . <EOS>\n",
      " Please close the door , please . <EOS>\n",
      " That 's what I could only say . <EOS>\n",
      " I 'll be right . <EOS>\n",
      " Tom and Mary know John are missing . <EOS>\n",
      " Why are you flinching ? <EOS>\n",
      " Go straight down . <EOS>\n",
      " Are you going to apologize ? <EOS>\n",
      " Tom was my first friend boyfriend . <EOS>\n",
      " The coffee is completely saturated . <EOS>\n",
      " I want to join the team . <EOS>\n",
      " Your plan sounds like a gunshot . <EOS>\n",
      " He was not impressed . <EOS>\n",
      " I put the cake on the list . <EOS>\n",
      "Epoch 16 ============================================================\n",
      "Total loss per word: 2.03865122795105\n",
      "Some generated sentences:\n",
      " You 're very kind . <EOS>\n",
      " I have n't slept well lately . <EOS>\n",
      " Mathematics is n't a virus . <EOS>\n",
      " You can go wherever you want to live here . <EOS>\n",
      " I asked my friend . <EOS>\n",
      " That 's probably the worst thing you can do . <EOS>\n",
      " Tom began fixing a pizza . <EOS>\n",
      " Tom told me to be more punctual . <EOS>\n",
      " I worked a very hard . <EOS>\n",
      " The organ sat on the bench . <EOS>\n",
      " I 'm not going to stand any longer . <EOS>\n",
      " I 'm going through something . <EOS>\n",
      " Could you please help Tom , please ? <EOS>\n",
      " Ah , that 's an important guess . <EOS>\n",
      " Tom washes hands once a day . <EOS>\n",
      "Epoch 17 ============================================================\n",
      "Total loss per word: 2.0009610652923584\n",
      "Some generated sentences:\n",
      " The soldiers have abated out-gunned . <EOS>\n",
      " Could you tell me what really happened ? <EOS>\n",
      " Can I rent rackets ? <EOS>\n",
      " Children often think they wish . <EOS>\n",
      " I do n't really know for a long time . <EOS>\n",
      " It feels weird . <EOS>\n",
      " Are you ready to go ? <EOS>\n",
      " Are you looking for your driver 's license ? <EOS>\n",
      " The picture was very strict with the baby . <EOS>\n",
      " I have to go over again . <EOS>\n",
      " I 've been looking for my apartment . <EOS>\n",
      " I was threatened . <EOS>\n",
      " He has a screw imagination . <EOS>\n",
      " This guy is very clever . <EOS>\n",
      " I did n't come here so late . <EOS>\n",
      "Epoch 18 ============================================================\n",
      "Total loss per word: 1.967815637588501\n",
      "Some generated sentences:\n",
      " Somebody seems to care for his requirements . <EOS>\n",
      " Show me what you have in your pocket . <EOS>\n",
      " I got this not for me 's stop . <EOS>\n",
      " I gave fined my word . <EOS>\n",
      " Find Tom . <EOS>\n",
      " All the eggs were taken . <EOS>\n",
      " We 're gon na have been equipement each other . <EOS>\n",
      " She is under a popular dress . <EOS>\n",
      " That does n't sound right . <EOS>\n",
      " I 'm going to study Chinese next year . <EOS>\n",
      " Do you feel like a drink ? <EOS>\n",
      " I ca n't function without you . <EOS>\n",
      " I 'm not the one who 's used to be . <EOS>\n",
      " She advised him to be more careful . <EOS>\n",
      " When did you go straight home ? <EOS>\n",
      "Epoch 19 ============================================================\n",
      "Total loss per word: 1.934528112411499\n",
      "Some generated sentences:\n",
      " We survived . <EOS>\n",
      " It 's all so complicated . <EOS>\n",
      " I kept on trying . <EOS>\n",
      " Let 's see what Tom did the other week . <EOS>\n",
      " Come on , let 's go . <EOS>\n",
      " It 's insanely likely to be late . Do n't ever let Tom out of the room . <EOS>\n",
      " Which browser do I cool ? <EOS>\n",
      " He has a lot of toys . <EOS>\n",
      " The world has unraveled . <EOS>\n",
      " We 're the same height . <EOS>\n",
      " It 's easier to do now than to go to sleep . <EOS>\n",
      " Tom cooks for me here . <EOS>\n",
      " What were you doing when I was in Boston ? <EOS>\n",
      " I feel very happy when I think . <EOS>\n",
      " She was very angry with me . <EOS>\n",
      "Traing done! Generating some more sentences.\n",
      " My friend helped me . <EOS>\n",
      " Trust me on this . <EOS>\n",
      " Are you doing something special on your birthday ? <EOS>\n",
      " She continued with her work . <EOS>\n",
      " I almost got wet . <EOS>\n",
      " Tom and Mary sat at the table tomorrow . <EOS>\n",
      " This glass is made of cardboard . <EOS>\n",
      " Tom is pretty good at chess . <EOS>\n",
      " This seems risky . <EOS>\n",
      " Could you please tell me again what school you graduated from ? <EOS>\n",
      " Would you like to rephrase the contract ? <EOS>\n",
      " No one can help you . <EOS>\n",
      " Do you like cheese ? <EOS>\n",
      " What is that ? <EOS>\n",
      " I really do n't feel like doing that by myself . <EOS>\n",
      " I enjoy it more fun . <EOS>\n",
      " You 're very pretty . <EOS>\n",
      " I fell asleep everywhere . <EOS>\n",
      " I told you count to 100 dollars . <EOS>\n",
      " It 's quite good . <EOS>\n",
      " While I was waiting for the bus , I took a week at the front stop and 15 error . <EOS>\n",
      " You 're not a teenager yet . <EOS>\n",
      " How do you know all these things ? <EOS>\n",
      " I read the article about you in yesterday 's newspaper . <EOS>\n",
      " I did n't do anything all day . <EOS>\n",
      " I think this is n't right . <EOS>\n",
      " She knit him a sweater . <EOS>\n",
      " He is afraid of death . <EOS>\n",
      " He 'd like to squish his bag , and please . <EOS>\n",
      " She asked him whether he 'd be able to attend the meeting . <EOS>\n",
      " I 'm pretty busy . <EOS>\n",
      " I appreciate what you did today . <EOS>\n",
      " I think I 've seen it . <EOS>\n",
      " They met for the first time in London . <EOS>\n",
      " All of us ate with Europe . <EOS>\n",
      " Take a picture with your dirty scored . <EOS>\n",
      " After I walked home , I was going to have to be very sleepy . <EOS>\n",
      " Did you see what Tom wrote ? <EOS>\n",
      " This one is ours . <EOS>\n",
      " She makes room for her children . <EOS>\n",
      " I can hear you . <EOS>\n",
      " Sorry , I did n't hear her . <EOS>\n",
      " I 'll get the message . <EOS>\n",
      " It would take you to get in on your business . <EOS>\n",
      " It 's snowing here . <EOS>\n",
      " I 'm not quite your maid . <EOS>\n",
      " I 'm going to lay back on the right . <EOS>\n",
      " Let 's throw all these stuff . <EOS>\n",
      " I heard a smoke being approved . <EOS>\n",
      " I 'll be happy for you . <EOS>\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss_sum = 0.0\n",
    "    total_word_count = 0.0\n",
    "    \n",
    "    for sentences in sentences_loader:\n",
    "\n",
    "        src_sentences = sentences['eng_sentences']\n",
    "        tgt_sentences = []\n",
    "\n",
    "        #Target sequence is source sequence shifted by one\n",
    "        for sentence in src_sentences:\n",
    "            tgt_sentences.append(sentence[1:])\n",
    "\n",
    "        for sent_idx in range(len(src_sentences)):\n",
    "            src_sentences[sent_idx] = src_sentences[sent_idx][:-1]\n",
    "    \n",
    "        #Create tensors from token lists\n",
    "        padded_src = pad_sequence(src_sentences, padding_value = 0, batch_first=True).to(device)\n",
    "        padded_tgt = pad_sequence(tgt_sentences, padding_value = 0, batch_first=True).to(device)\n",
    "\n",
    "        src_padding_mask = get_padding_mask(padded_src)\n",
    "        src_subsq_mask = get_square_subsequent_mask(padded_src.size()[1])\n",
    "\n",
    "        pred = transformer_model.forward(\n",
    "            src = padded_src,\n",
    "            src_padding_mask = src_padding_mask,\n",
    "            src_subsq_mask = src_subsq_mask\n",
    "        )\n",
    "\n",
    "        #Mask to zero one hot vectors corresponding to padded elements\n",
    "        one_hot_mask = get_padding_mask(padded_tgt, val1 = float(0.0), val2 = float(1.0))\n",
    "        y_one_hot = get_one_hot(padded_tgt.squeeze(dim=2), in_dict_size, mask = one_hot_mask)\n",
    "\n",
    "        loss = - torch.sum(torch.log(pred) * y_one_hot)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss_sum += loss.detach().to('cpu').data\n",
    "        total_word_count += torch.sum(y_one_hot).to('cpu').data\n",
    "        \n",
    "    print(f\"Epoch {epoch} \" + '=' * 60)\n",
    "    print(f\"Total loss per word: {train_loss_sum / total_word_count}\")\n",
    "    print(f\"Some generated sentences:\")\n",
    "    generate_some_sentences(num_sentences = 15)\n",
    "    \n",
    "    if STORE_MODELS == True:\n",
    "        model_path = os.path.join(models_path, f'Epoch_{epoch}_model.pt')\n",
    "        torch.save(transformer_model, model_path)\n",
    "\n",
    "print(\"Traing done! Generating some more sentences.\")\n",
    "generate_some_sentences(num_sentences = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
