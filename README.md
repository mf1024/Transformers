# Transformers

### Here are two notebooks of transformer implementation walk-troughs:

- Transformer encoder for Language Modeling implementation in PyTorch [jupyter notebook](https://github.com/mf1024/Transformers/blob/master/Transformer%20Encoder%20for%20Language%20Modeling.ipynb)

- Transformer for Language Translation in PyTorch [jupyter notebook](https://github.com/mf1024/Transformers/blob/master/Transformer.ipynb)


### And here are two notebooks of using pretrained GPT-2 model:

- Generating text with a pre-trained GPT2 in PyTorch [jupyter notebook](https://github.com/mf1024/Transformers/blob/master/Generating%20text%20with%20a%20pre-trained%20GPT2%20model.ipynb)

- Fine-tuning GPT-2 on a jokes dataset in PyTorch [jupyter notebook](https://github.com/mf1024/Transformers/blob/master/Fine-tuning%20GPT2-medium%20in%20PyTorch.ipynb)


### I also wrote a blog post about fine-tuning pre-trained GPT2 model [here](https://mf1024.github.io/2019/11/12/Fun-With-GPT-2_/)
